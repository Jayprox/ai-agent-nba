# backend/test_narrative_contract.py
"""
Contract test for /nba/narrative/today endpoint.
Runs both template and AI modes, validates schema keys,
and logs basic performance metrics.
"""

import json
import time
import requests
from datetime import datetime

BASE_URL = "http://127.0.0.1:8000/nba/narrative/today"


def _check_keys(data, required_keys):
    """Return list of missing keys (if any)."""
    return [k for k in required_keys if k not in data]


def run_contract_test(mode="template"):
    """Run a single test mode and validate."""
    print(f"\nğŸ§ª Testing mode: {mode.upper()}")

    t0 = time.perf_counter()
    resp = requests.get(BASE_URL, params={"mode": mode, "cache_ttl": 0})
    latency_ms = round((time.perf_counter() - t0) * 1000, 2)

    if resp.status_code != 200:
        print(f"âŒ HTTP {resp.status_code} â€” {resp.text[:200]}")
        return False

    try:
        payload = resp.json()
    except json.JSONDecodeError:
        print("âŒ Response was not valid JSON.")
        return False

    # ---------------- Schema validation ----------------
    required_top = ["ok", "summary", "raw", "mode"]
    missing_top = _check_keys(payload, required_top)
    if missing_top:
        print("âŒ Missing top-level keys:", missing_top)
        return False

    summary = payload.get("summary", {})
    required_summary = ["metadata"]
    missing_summary = _check_keys(summary, required_summary)
    if missing_summary:
        print("âŒ Missing summary keys:", missing_summary)
        return False

    metadata = summary.get("metadata", {})
    required_meta = ["generated_at", "model"]
    missing_meta = _check_keys(metadata, required_meta)
    if missing_meta:
        print("âŒ Missing metadata keys:", missing_meta)
        return False

    raw = payload.get("raw", {})
    meta_raw = raw.get("meta", {})
    if meta_raw:
        latency = meta_raw.get("latency_ms", latency_ms)
        cache = meta_raw.get("cache_used", False)
        print(
            f"âœ… {mode.upper()} PASS â€” "
            f"latency: {latency:.2f} ms | cache_used: {cache} | "
            f"model: {metadata.get('model')}"
        )
    else:
        print(
            f"âœ… {mode.upper()} PASS â€” "
            f"latency: {latency_ms:.2f} ms | model: {metadata.get('model')}"
        )

    # Optional debug dump for detailed logs
    print(json.dumps(metadata, indent=2))
    return True


if __name__ == "__main__":
    print("ğŸ§­ NBA Narrative Contract Test")
    print(f"â±ï¸  Started: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')}")

    ok_template = run_contract_test("template")
    ok_ai = run_contract_test("ai")

    print("\nğŸ“Š Summary Report")
    print(f"Template mode: {'âœ… PASS' if ok_template else 'âŒ FAIL'}")
    print(f"AI mode: {'âœ… PASS' if ok_ai else 'âŒ FAIL'}")

    if ok_template and ok_ai:
        print("\nğŸ‰ All narrative contract tests passed successfully.")
    else:
        print("\nâš ï¸  One or more tests failed.")
